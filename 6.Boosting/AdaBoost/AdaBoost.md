## AdaBoost

















### 周志华讲座

#### 关于AdaBoost讲座

1989年AdaBoost方法被提出，目前被广泛应用各个领域，它主要有一下优点：

* 作为一种集成方法的思想被很多领域所借鉴

* 根据AdaBoost一些具体的实现方法不同，演化出算法族，因此它包含了各种算法

* 有理论的证明，解释它为什么有效

  泛化误差：$\epsilon_D\le\epsilon_D+O(\sqrt{\frac{dT}m})$

  ​	$d$:复杂度，$T$:训练轮数，$m$:模型个数

试验结果表明：AdaBoost不会过拟合，即在训练误差达到最小后，随着继续的训练，泛化误差不升反降（这是一个非常优秀的特点，找到原因可以对机器学习算法的研究起到很大作用），这是为什么呢？

主要的解释分为两种流派，第一种是统计学派（Statistical View）,统计学派对机器学习的思路是找一个目标函数，然后用最优化的思想去优化这个目标函数。（具体关于AdaBoost不会过拟合的解释周老师没有讲太清楚，他着重于第二个观点）。

第二种是间隔理论（Margin Theory）,这种解释有点类似SVM的方法，它认为训练误差为0只是找到了一条将训练数据集划分开的线，继续训练的过程相当于把数据集与这条线的距离增大，所以泛化误差下降。

具体公式推导见[AdaBoost(周志华)](AdaBoost(周志华).pdf)



#### 关于深度学习

他认为近些年深度神经网络得以在强大的算力下发挥巨大作用的最重要原因是解决了梯度消失的问题

深度学习实际上是一系列类线性模型的叠加，它相当于对一个复杂函数的拟合，深度学习的使用需要数值建模，在训练过程中需要通过反向传播算法求出梯度。周志华提出了深度森林的算法，相当于把若干个子模型的叠加，实质上是提供了一个大的框架。