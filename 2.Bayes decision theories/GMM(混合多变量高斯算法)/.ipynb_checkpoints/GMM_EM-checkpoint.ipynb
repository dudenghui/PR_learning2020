{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA#主成分分析\n",
    "from sklearn.mixture import GaussianMixture,BayesianGaussianMixture\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4) (75, 4) (75,) (75,)\n",
      "IRIS:Number of mislabeled points out of a total 75 points : 0, Acc: 100.000000%\n"
     ]
    }
   ],
   "source": [
    "class GMM():\n",
    "    def __init__(self,n_components = 5, max_iter = 10):\n",
    "        self.k = n_components\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def EM_GMM(self, X, param, eps=1e-5, max_iter=100):\n",
    "        theta = {}\n",
    "        theta['pi'] = np.ones(param['k'])/param['k']                 # 均匀初始化\n",
    "        theta['mu'] = np.random.random((param['k'],param['dim']))    # 随机初始化\n",
    "        theta['sigma'] = np.array([np.eye(param['dim'])]*param['k']) # 初始化为单位正定矩阵\n",
    "        \n",
    "        P_mat = np.zeros((len(X), param['k']))  # 概率矩阵\n",
    "        for i in range(max_iter):# 迭代次数\n",
    "            for k in range(param['k']):\n",
    "                theta['sigma'][k, ...] += self.regularization\n",
    "                P_mat[:, k] = theta['pi'][k]*multivariate_normal(theta['mu'][k], theta['sigma'][k, ...], allow_singular = True).pdf(X)\n",
    "            \n",
    "            totol_N = P_mat.sum(axis=1)  # 计算各样本出现的总频率\n",
    "            totol_N[totol_N == 0] = param['k']# 如果某一样本在各类中的出现频率和为0，则使用K来代替，相当于分配等概率\n",
    "            P_mat /= totol_N.reshape(-1, 1)\n",
    "            #### M-step，更新参数 ####\n",
    "            for k in range(param['k']):\n",
    "                Nk = np.sum(P_mat[:, k], axis=0)  # 类出现的频率\n",
    "                theta['pi'][k] = Nk / len(X)\n",
    "                theta['mu'][k] = (1 / Nk) * np.sum(X *P_mat[:, k].reshape(-1, 1), axis=0)  # 该类的新均值\n",
    "                theta['sigma'][k] = (1 / Nk) * np.dot((P_mat[:, k].reshape(-1, 1)* (X - theta['mu'][k])).T,\n",
    "                                                          (X - theta['mu'][k]))\n",
    "            return theta    \n",
    "\n",
    "    def train(self, X, y):\n",
    "        param = {}\n",
    "        param['k'] = self.k; param['N'] = X.shape[0]; param['dim'] = X.shape[1]#这里的N是整个训练样本的样本数，在实际计算时取相同标签的样本数\n",
    "        \n",
    "        self.regularization = np.dot(np.eye(param['dim']),0.001)\n",
    "  \n",
    "        labels = list(set(y))#标签的列表\n",
    "        data = {label:[] for label in labels}#{0.0: [], 1.0: []}\n",
    "        for f, label in zip(X, y):\n",
    "            data[label].append(f)#print(data)#形成一个字典，根据标签将训练样本进行分类\n",
    "       \n",
    "        self.model = {label:{} for label in range(len(labels))}        \n",
    "        for i in range(len(labels)):\n",
    "            self.model[i] = self.EM_GMM(data[i],param,eps=1e-5, max_iter=self.max_iter)\n",
    "            \n",
    "        return self.model\n",
    "\n",
    "    # 计算概率\n",
    "    def calculate_probabilities(self, input_data):\n",
    "        probabilities = {}\n",
    "        dim = np.size(input_data)\n",
    "        \n",
    "        for label, value in self.model.items():#value是一个字典，表示特定标签的模型参数\n",
    "            pp = 0.0\n",
    "            for i in range(self.k):\n",
    "                mu,sigma = value['mu'][i],value['sigma'][i]\n",
    "                pp += multivariate_normal(mu, sigma, allow_singular = True).pdf(input_data)*value['pi'][i]\n",
    "            probabilities[label] = pp\n",
    "            \n",
    "        return probabilities\n",
    "    \n",
    "    # 类别\n",
    "    def predict(self, X_test):\n",
    "        label = list(range(X_test.shape[0]))\n",
    "        for i in range(X_test.shape[0]):#每个样本迭代一次\n",
    "            label[i] = sorted(self.calculate_probabilities(X_test[i,:]).items(), key=lambda x: x[-1])[-1][0]\n",
    "        \n",
    "        return label\n",
    "    \n",
    "    \n",
    "iris = datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "model = GMM(n_components=5, max_iter = 10)\n",
    "model_0 = model.train(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"IRIS:Number of mislabeled points out of a total %d points : %d, Acc: %f%%\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum(),100*(y_test == y_pred).sum()/X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self,n_components = 5, max_iter = 100):\n",
    "        self.k = 3\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \n",
    "        labels = list(set(y))#标签的列表\n",
    "        data = {label:[] for label in labels}#{0.0: [], 1.0: []}\n",
    "        for f, label in zip(X, y):\n",
    "            data[label].append(f)#print(data)#形成一个字典，根据标签将训练样本进行分类\n",
    "       \n",
    "        self.model = {label:{} for label in range(len(labels))}\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            dpgmm = BayesianGaussianMixture(n_components=self.k).fit(data[i])\n",
    "            self.model[i]['pi'] = dpgmm.weights_\n",
    "            self.model[i]['mu'] = dpgmm.means_\n",
    "            self.model[i]['sigma'] = dpgmm.covariances_\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    # 计算概率\n",
    "    def calculate_probabilities(self, input_data):\n",
    "        probabilities = {}\n",
    "        dim = np.size(input_data)\n",
    "        \n",
    "        for label, value in self.model.items():#value是一个字典，表示特定标签的模型参数\n",
    "            pp = 0.0\n",
    "            for i in range(self.k):\n",
    "                mu,sigma = value['mu'][i],value['sigma'][i]\n",
    "                pp += multivariate_normal(mu,sigma).pdf(input_data)*value['pi'][i]\n",
    "            probabilities[label] = pp\n",
    "            \n",
    "        return probabilities\n",
    "    \n",
    "    # 类别\n",
    "    def predict(self, X_test):\n",
    "        label = list(range(X_test.shape[0]))\n",
    "        for i in range(X_test.shape[0]):#每个样本迭代一次\n",
    "            label[i] = sorted(self.calculate_probabilities(X_test[i,:]).items(), key=lambda x: x[-1])[-1][0]\n",
    "        \n",
    "        return label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合高斯分类MNIST数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:85: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:85: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA#主成分分析\n",
    "from sklearn.datasets import fetch_mldata\n",
    " \n",
    "mnist = fetch_mldata('MNIST original',data_home=\".\\Mnist\")\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "X_reduced = PCA(n_components=50).fit_transform(X)#特征提取\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_reduced, y, test_size=0.1, random_state=1)#random_state=1使数据集顺序打乱\n",
    "# X_train_d = (X_train_d-np.min(X_train_d))/(np.max(X_train_d)-np.min(X_train_d))\n",
    "# X_test_d = (X_test_d-np.min(X_test_d))/(np.max(X_test_d)-np.min(X_test_d))\n",
    "\n",
    "model_d = GMM(n_components=2)\n",
    "model_d.train(X_train_d, y_train_d)\n",
    "y_pred_d = model_d.predict(X_test_d)\n",
    "\n",
    "print(\"MNIST:Number of mislabeled points out of a total %d points : %d, Acc: %f%%\"\n",
    "      %(X_test_d.shape[0], (y_test_d != y_pred_d).sum(),100*(y_test_d == y_pred_d).sum()/X_test_d.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合高斯分类CIFAR10数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR:Number of mislabeled points out of a total 10000 points : 8370, Acc: 16.300000%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "cifar_batch = {}\n",
    "cifar_batch[0] = unpickle(\".\\CIFAR10\\data_batch_5\")\n",
    "cifar_batch[1] = unpickle(\".\\CIFAR10\\data_batch_1\")\n",
    "cifar_batch[2] = unpickle(\".\\CIFAR10\\data_batch_2\")\n",
    "cifar_batch[3] = unpickle(\".\\CIFAR10\\data_batch_3\")\n",
    "cifar_batch[4] = unpickle(\".\\CIFAR10\\data_batch_4\")\n",
    "cifar_test = unpickle(\".\\CIFAR10\\\\test_batch\")\n",
    "cifar_batch_meta = unpickle(\".\\CIFAR10\\\\batches.meta\")\n",
    "\n",
    "model_c = GMM(n_components=5)\n",
    "for i in range(5):\n",
    "    X_c,y_c = cifar_batch[i][b'data'],cifar_batch[i][b'labels']\n",
    "    X_c = X_c/255#对图片数据进行归一化\n",
    "    y_c = np.array(y_c)#将y_c转化为numpy数组\n",
    "    X_reduced_c = PCA(n_components=30).fit_transform(X_c)#特征提取\n",
    "#     X_reduced_c = (X_reduced_c-np.min(X_reduced_c))/(np.max(X_reduced_c)-np.min(X_reduced_c))\n",
    "    model_c.train(X_reduced_c, y_c)\n",
    "\n",
    "X_test,y_test = cifar_test[b'data'],cifar_test[b'labels']\n",
    "X_test = X_test/255\n",
    "y_test_c = np.array(y_test)\n",
    "X_test_c = PCA(n_components=30).fit_transform(X_test)#特征提取\n",
    "# X_test_c = (X_test_c-np.min(X_test_c))/(np.max(X_test_c)-np.min(X_test_c))#数据归一化\n",
    "y_pred_c = model_c.predict(X_test_c)\n",
    "\n",
    "print(\"CIFAR:Number of mislabeled points out of a total %d points : %d, Acc: %f%%\"\n",
    "      %(X_test_c.shape[0], (y_test_c != y_pred_c).sum(),100*(y_test_c == y_pred_c).sum()/X_test_c.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用混合高斯对CIFAR10分类精度过低（挠头）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
