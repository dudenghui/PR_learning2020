{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA#主成分分析\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.mixture import GaussianMixture,BayesianGaussianMixture\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们谈到了用 k-means 进行聚类的方法，这次我们来说一下另一个很流行的算法：Gaussian Mixture Model (GMM)。事实上，GMM 和 k-means 很像，不过 GMM 是学习出一些概率密度函数来（所以 GMM 除了用在 clustering 上之外，还经常被用于 density estimation ），简单地说，k-means 的结果是每个数据点被 assign 到其中某一个 cluster 了，而 GMM 则给出这些数据点被 assign 到每个 cluster 的概率，又称作 soft assignment 。\n",
    "\n",
    "得出一个概率有很多好处，因为它的信息量比简单的一个结果要多，比如，我可以把这个概率转换为一个 score ，表示算法对自己得出的这个结果的把握。也许我可以对同一个任务，用多个方法得到结果，最后选取“把握”最大的那个结果；另一个很常见的方法是在诸如疾病诊断之类的场所，机器对于那些很容易分辨的情况（患病或者不患病的概率很高）可以自动区分，而对于那种很难分辨的情况，比如，49% 的概率患病，51% 的概率正常，如果仅仅简单地使用 50% 的阈值将患者诊断为“正常”的话，风险是非常大的，因此，在机器对自己的结果把握很小的情况下，会“拒绝发表评论”，而把这个任务留给有经验的医生去解决。\n",
    "\n",
    "废话说了一堆，不过，在回到 GMM 之前，我们再稍微扯几句。我们知道，不管是机器还是人，学习的过程都可以看作是一种“归纳”的过程，在归纳的时候你需要有一些假设的前提条件，例如，当你被告知水里游的那个家伙是鱼之后，你使用“在同样的地方生活的是同一种东西”这类似的假设，归纳出“在水里游的都是鱼”这样一个结论。当然这个过程是完全“本能”的，如果不仔细去想，你也不会了解自己是怎样“认识鱼”的。另一个值得注意的地方是这样的假设并不总是完全正确的，甚至可以说总是会有这样那样的缺陷的，因此你有可能会把虾、龟、甚至是潜水员当做鱼。也许你觉得可以通过修改前提假设来解决这个问题，例如，基于“生活在同样的地方并且穿着同样衣服的是同一种东西”这个假设，你得出结论：在水里有并且身上长有鳞片的是鱼。可是这样还是有问题，因为有些没有长鳞片的鱼现在又被你排除在外了。\n",
    "\n",
    "在这个问题上，机器学习面临着和人一样的问题，在机器学习中，一个学习算法也会有一个前提假设，这里被称作“归纳偏执 (bias)”（bias 这个英文词在机器学习和统计里还有其他许多的意思）。例如线性回归，目的是要找一个函数尽可能好地拟合给定的数据点，它的归纳偏执就是“满足要求的函数必须是线性函数”。一个没有归纳偏执的学习算法从某种意义上来说毫无用处，就像一个完全没有归纳能力的人一样，在第一次看到鱼的时候有人告诉他那是鱼，下次看到另一条鱼了，他并不知道那也是鱼，因为两条鱼总有一些地方不一样的，或者就算是同一条鱼，在河里不同的地方看到，或者只是看到的时间不一样，也会被他认为是不同的，因为他无法归纳，无法提取主要矛盾、忽略次要因素，只好要求所有的条件都完全一样──然而哲学家已经告诉过我们了：世界上不会有任何样东西是完全一样的，所以这个人即使是有无比强悍的记忆力，也绝学不到任何一点知识。\n",
    "\n",
    "这个问题在机器学习中称作“过拟合 (Overfitting)”，例如前面的回归的问题，如果去掉“线性函数”这个归纳偏执，因为对于 N 个点，我们总是可以构造一个 N-1 次多项式函数，让它完美地穿过所有的这 N 个点，或者如果我用任何大于 N-1 次的多项式函数的话，我甚至可以构造出无穷多个满足条件的函数出来。如果假定特定领域里的问题所给定的数据个数总是有个上限的话，我可以取一个足够大的 N ，从而得到一个（或者无穷多个）“超级函数”，能够 fit 这个领域内所有的问题。然而这个（或者这无穷多个）“超级函数”有用吗？只要我们注意到学习的目的（通常）不是解释现有的事物，而是从中归纳出知识，并能应用到新的事物上，结果就显而易见了。\n",
    "\n",
    "没有归纳偏执或者归纳偏执太宽泛会导致 Overfitting ，然而另一个极端──限制过大的归纳偏执也是有问题的：如果数据本身并不是线性的，强行用线性函数去做回归通常并不能得到好结果。难点正在于在这之间寻找一个平衡点。不过人在这里相对于（现在的）机器来说有一个很大的优势：人通常不会孤立地用某一个独立的系统和模型去处理问题，一个人每天都会从各个来源获取大量的信息，并且通过各种手段进行整合处理，归纳所得的所有知识最终得以统一地存储起来，并能有机地组合起来去解决特定的问题。这里的“有机”这个词很有意思，搞理论的人总能提出各种各样的模型，并且这些模型都有严格的理论基础保证能达到期望的目的，然而绝大多数模型都会有那么一些“参数”（例如 K-means 中的 k ），通常没有理论来说明参数取哪个值更好，而模型实际的效果却通常和参数是否取到最优值有很大的关系，我觉得，在这里“有机”不妨看作是所有模型的参数已经自动地取到了最优值。另外，虽然进展不大，但是人们也一直都期望在计算机领域也建立起一个统一的知识系统（例如语意网就是这样一个尝试）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.det(np.eye(3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4) (75, 4) (75,) (75,)\n",
      "{0: {'pi': array([0.82536018, 0.17463982]), 'mu': array([[6.52287126, 3.07037811, 5.54832577, 2.08188495],\n",
      "       [5.9437815 , 2.71718001, 5.19692111, 1.71258978]]), 'sigma': array([[[ 0.36095862,  0.05615942,  0.2432668 ,  0.03546897],\n",
      "        [ 0.05615942,  0.06906184,  0.05887008,  0.05427358],\n",
      "        [ 0.2432668 ,  0.05887008,  0.21734134,  0.04740138],\n",
      "        [ 0.03546897,  0.05427358,  0.04740138,  0.08054766]],\n",
      "\n",
      "       [[ 0.05357667, -0.00211986,  0.02835285, -0.05003927],\n",
      "        [-0.00211986,  0.00838199, -0.01653364,  0.00841736],\n",
      "        [ 0.02835285, -0.01653364,  0.05747275, -0.04601809],\n",
      "        [-0.05003927,  0.00841736, -0.04601809,  0.05772274]]])}, 1: {'pi': array([0.82536018, 0.17463982]), 'mu': array([[6.52287126, 3.07037811, 5.54832577, 2.08188495],\n",
      "       [5.9437815 , 2.71718001, 5.19692111, 1.71258978]]), 'sigma': array([[[ 0.36095862,  0.05615942,  0.2432668 ,  0.03546897],\n",
      "        [ 0.05615942,  0.06906184,  0.05887008,  0.05427358],\n",
      "        [ 0.2432668 ,  0.05887008,  0.21734134,  0.04740138],\n",
      "        [ 0.03546897,  0.05427358,  0.04740138,  0.08054766]],\n",
      "\n",
      "       [[ 0.05357667, -0.00211986,  0.02835285, -0.05003927],\n",
      "        [-0.00211986,  0.00838199, -0.01653364,  0.00841736],\n",
      "        [ 0.02835285, -0.01653364,  0.05747275, -0.04601809],\n",
      "        [-0.05003927,  0.00841736, -0.04601809,  0.05772274]]])}, 2: {'pi': array([0.82536018, 0.17463982]), 'mu': array([[6.52287126, 3.07037811, 5.54832577, 2.08188495],\n",
      "       [5.9437815 , 2.71718001, 5.19692111, 1.71258978]]), 'sigma': array([[[ 0.36095862,  0.05615942,  0.2432668 ,  0.03546897],\n",
      "        [ 0.05615942,  0.06906184,  0.05887008,  0.05427358],\n",
      "        [ 0.2432668 ,  0.05887008,  0.21734134,  0.04740138],\n",
      "        [ 0.03546897,  0.05427358,  0.04740138,  0.08054766]],\n",
      "\n",
      "       [[ 0.05357667, -0.00211986,  0.02835285, -0.05003927],\n",
      "        [-0.00211986,  0.00838199, -0.01653364,  0.00841736],\n",
      "        [ 0.02835285, -0.01653364,  0.05747275, -0.04601809],\n",
      "        [-0.05003927,  0.00841736, -0.04601809,  0.05772274]]])}}\n",
      "IRIS:Number of mislabeled points out of a total 75 points : 48, Acc: 36.000000%\n"
     ]
    }
   ],
   "source": [
    "class GMM:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.k = 2\n",
    "        \n",
    "    def GMM_component(self, X, theta,param, k):#计算正态分布概率\n",
    "        #这里要判断sigma是否为奇异阵，如果是奇异阵那么需要添加一个正则项\n",
    "#         if(np.linalg.det(theta['sigma'][k])==0.0):\n",
    "#             theta['sigma'][k] = self.regularization\n",
    "            \n",
    "#         sign = X-theta['mu'][k]\n",
    "#         exponent = math.exp((-0.5*np.dot(sign.T,np.dot(np.linalg.inv(theta['sigma'][k]),sign))))\n",
    "#         pp = exponent/(((2*math.pi)**(param[\"dim\"]/2))*(np.linalg.det(theta['sigma'][k])**0.5))\n",
    "#         return theta['pi'][k]*pp\n",
    "\n",
    "        for i in range(param['dim']):\n",
    "            for j in range(param['dim']):\n",
    "                if np.isnan(theta['sigma'][k,i,j]):\n",
    "                    theta['sigma'][k,i,j] = 0\n",
    "        return theta['pi'][k]*multivariate_normal(theta['mu'][k], theta['sigma'][k,:,:],allow_singular=1,seed=1).pdf(X)\n",
    "        \n",
    "    def E_step(self, theta, param,X):#E步：更新隐变量概率分布q(Z)。这里的X是一个样本\n",
    "        q = np.zeros((param['k'],len(X)))\n",
    "        for i in range(param['k']):\n",
    "            for j in range(len(X)):\n",
    "                q[i,j] = self.GMM_component(X[j], theta ,param, i)\n",
    "            \n",
    "        q /= q.sum(axis=0)\n",
    "#         print(q)\n",
    "        return q\n",
    "\n",
    "    def M_step(self,X,q,theta,param):#M步：使用q(Z)更新GMM参数。\n",
    "        pi_temp = q.sum(axis=1); \n",
    "        pi_temp /= len(X) # 计算pi\n",
    "        mu_temp = q.dot(X); mu_temp /= q.sum(axis=1)[:, None] # 计算mu\n",
    "        sigma_temp = np.zeros((param['k'], param['dim'], param['dim']))\n",
    "        for i in range(param['k']):\n",
    "            ys = X - mu_temp[i, :]\n",
    "            sigma_temp[i] = np.sum(q[i, :, None, None]*np.matmul(ys[..., None], ys[:, None, :]), axis=0)\n",
    "        sigma_temp /= np.sum(q, axis=1)[:, None, None] # 计算sigma\n",
    "        theta['pi'] = pi_temp; theta['mu'] = mu_temp; theta['sigma'] = sigma_temp\n",
    "        return theta\n",
    "\n",
    "    def likelihood(self,X,theta,param):#计算GMM的对数似然。\n",
    "        ll = 0\n",
    "        for i in range(param['k']):\n",
    "            ll += self.GMM_component(X[0],theta,param,i)\n",
    "        ll = np.log(ll).sum()\n",
    "        return ll\n",
    "\n",
    "    def EM_GMM(self,X,theta,param,eps=1e-5,max_iter=1000):#eps: 计算精度; max_iter: 最大迭代次数\n",
    "        #print(X)#是一个由array构成的list\n",
    "        for i in range(max_iter):\n",
    "            ll_old = 0\n",
    "            q = self.E_step(theta, param,X)# E-step\n",
    "            theta = self.M_step(X, q, theta, param)# M-step\n",
    "            ll_new = self.likelihood(X, theta,param)\n",
    "            if np.abs(ll_new - ll_old) < eps:\n",
    "                break;\n",
    "            else:\n",
    "                ll_old = ll_new\n",
    "        ll_new = 1\n",
    "        return theta\n",
    "\n",
    "    # 分类别求出数学期望和标准差\n",
    "    def train(self, X, y):\n",
    "        theta = {}; param = {}\n",
    "        param['k'] = self.k; param['N'] = X.shape[0]; param['dim'] = X.shape[1]#这里的N是整个训练样本的样本数，在实际计算时取相同标签的样本数\n",
    "        theta['pi'] = np.ones(param['k'])/param['k']                 # 均匀初始化\n",
    "        theta['mu'] = np.random.random((param['k'],param['dim']))    # 随机初始化\n",
    "        theta['sigma'] = np.array([np.eye(param['dim'])]*param['k']) # 初始化为单位正定矩阵\n",
    "        self.regularization = np.dot(np.eye(param['dim']),0.001)\n",
    "#         print( self.regularization)\n",
    "        #print(theta)\n",
    "        \n",
    "        labels = list(set(y))#标签的列表\n",
    "        data = {label:[] for label in labels}#{0.0: [], 1.0: []}\n",
    "        for f, label in zip(X, y):\n",
    "            data[label].append(f)#print(data)#形成一个字典，根据标签将训练样本进行分类\n",
    "        #print(data)\n",
    "        \n",
    "        self.model = {label: self.EM_GMM(value,theta,param,eps=1e-5,max_iter=50) for label, value in data.items()}\n",
    "        print(self.model)\n",
    "        return self.model\n",
    "\n",
    "    # 计算概率\n",
    "    def calculate_probabilities(self, input_data):\n",
    "        probabilities = {}\n",
    "        pp = 0.0\n",
    "        dim = np.size(input_data)\n",
    "        for label, value in self.model.items():#value是一个字典，表示特定标签的模型参数\n",
    "#             print(value['pi'][1])\n",
    "            for i in range(self.k):\n",
    "                mu,sigma = value['mu'][i],value['sigma'][i]\n",
    "#                 sign = input_data-mu\n",
    "#                 exponent = math.exp((-0.5*np.dot(sign,np.dot(np.linalg.inv(sigma),sign.T))))\n",
    "#                 pp = exponent/(((2*math.pi)**(dim/2))*(np.linalg.det(sigma)**0.5))\n",
    "#                 for i in range(4):\n",
    "#                     for j in range(4):\n",
    "#                         if np.isnan(sigma[i,j]):\n",
    "#                             sigma[i,j] = 0\n",
    "                            \n",
    "                pp += multivariate_normal(mu,sigma,allow_singular=1,seed=1).pdf(input_data)*value['pi'][i]\n",
    "#                 pp = self.GMM_component(self,input_data,self.model[i]['theta'],self.model[i]['param'],i)\n",
    "#                 probabilities[label] += value['pi'][i]*pp\n",
    "            probabilities[label] = pp\n",
    "            pp = 0.0\n",
    "\n",
    "        return probabilities\n",
    "    \n",
    "    # 类别\n",
    "    def predict(self, X_test):\n",
    "        label = list(range(X_test.shape[0]))\n",
    "        for i in range(X_test.shape[0]):#每个样本迭代一次\n",
    "            label[i] = sorted(self.calculate_probabilities(X_test[i,:]).items(), key=lambda x: x[-1])[-1][0]\n",
    "        \n",
    "        return label\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        right = 0\n",
    "        for X, y in zip(X_test, y_test):\n",
    "            label = self.predict(X)\n",
    "            if label == y:\n",
    "                right += 1\n",
    "\n",
    "        return right / float(len(X_test))\n",
    "    \n",
    "iris = datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "model = GMM()\n",
    "model.train(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"IRIS:Number of mislabeled points out of a total %d points : %d, Acc: %f%%\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum(),100*(y_test == y_pred).sum()/X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4) (75, 4) (75,) (75,)\n",
      "(26, 4)\n",
      "(26, 4)\n",
      "(23, 4)\n",
      "[4 3 1 0]\n",
      "IRIS:Number of mislabeled points out of a total 75 points : 75, Acc: 0.000000%\n"
     ]
    }
   ],
   "source": [
    "class GaussianMixture:\n",
    "    def __init__(self, n_components: int = 1, covariance_type: str = 'full',\n",
    "                 tol: float = 0.001, reg_covar: float = 1e-06, max_iter: int = 100):\n",
    "        self.n_components = n_components#混合模型的个数\n",
    "        self.means_ = None\n",
    "        self.covariances_ = None\n",
    "        self.weights_ = None\n",
    "        self.reg_covar = reg_covar  # 该参数是为了防止出现奇异协方差矩阵\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def EM_GMM(self, X_train):\n",
    "        # 获取一些必要的数据信息\n",
    "        n_samples, n_feature = X_train.shape\n",
    "        self.reg_covar = self.reg_covar * np.identity(n_feature)\n",
    "\n",
    "        # 初始化一些必要的参数：均值，协方差，权重\n",
    "        self.means_ = np.random.randint(X_train.min()/2, X_train.max()/2, size=(self.n_components, n_feature))\n",
    "        self.covariances_ = np.zeros((self.n_components, n_feature, n_feature))\n",
    "        for k in range(self.n_components):\n",
    "            np.fill_diagonal(self.covariances_[k], 1)\n",
    "        self.weights_ = np.ones(self.n_components) / self.n_components\n",
    "\n",
    "        P_mat = np.zeros((n_samples, self.n_components))  # 概率矩阵\n",
    "        for i in range(self.max_iter):# 分别对K各类概率\n",
    "            for k in range(self.n_components):\n",
    "                self.covariances_ += self.reg_covar  # 防止出现奇异协方差矩阵\n",
    "                g = multivariate_normal(mean=self.means_[k], cov=self.covariances_[k])\n",
    "                #### E-step，计算概率 ####\n",
    "                P_mat[:, k] = self.weights_[k] * g.pdf(X_train)  # 计算X在各分布下出现的频率\n",
    "            totol_N = P_mat.sum(axis=1)  # 计算各样本出现的总频率\n",
    "            totol_N[totol_N == 0] = self.n_components# 如果某一样本在各类中的出现频率和为0，则使用K来代替，相当于分配等概率\n",
    "            P_mat /= totol_N.reshape(-1, 1)\n",
    "           \n",
    "            #### M-step，更新参数 ####\n",
    "            for k in range(self.n_components):\n",
    "                N_k = np.sum(P_mat[:, k], axis=0)  # 类出现的频率\n",
    "                self.means_[k] = (1/N_k) * np.sum(X_train *P_mat[:, k].reshape(-1, 1), axis=0)  # 该类的新均值\n",
    "                self.covariances_[k]=(1/N_k)*np.dot((P_mat[:, k].reshape(-1, 1)* (X_train - self.means_[k])).T,\n",
    "                                                          (X_train - self.means_[k])) + self.reg_covar\n",
    "                self.weights_[k] = N_k / n_samples\n",
    "        \n",
    "        theta = {}\n",
    "        theta['pi'] = self.weights_\n",
    "        theta['mu'] = self.means_\n",
    "        theta['sigma'] = self.covariances_\n",
    "        return theta\n",
    "    def train(self, X, y):\n",
    "        self.model = []\n",
    "        labels = list(set(y))#标签的列表\n",
    "        self.lable_num = len(labels)\n",
    "        for n,label in enumerate(labels):\n",
    "            input_x = X[y==label]\n",
    "            print(input_x.shape)\n",
    "            self.model.append(self.EM_GMM(input_x))\n",
    "#         print(self.model)\n",
    "        print(self.model[0]['mu'][1])\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    # 计算概率\n",
    "    def calculate_probabilities(self, input_data):\n",
    "        p = []\n",
    "        for i in range(self.lable_num):\n",
    "            for j in range(self.n_components):\n",
    "                g = multivariate_normal(mean=self.model[i]['mu'][j], cov=self.model[i]['sigma'][j])\n",
    "            p.append( self.model[i]['pi'] * g.pdf(input_data))\n",
    "    \n",
    "        return p#输出一个数组，表示每一类的概率\n",
    "    \n",
    "    # 类别\n",
    "    def predict(self, X_test):\n",
    "        label = list(range(X_test.shape[0]))\n",
    "        for i in range(X_test.shape[0]):#每个样本迭代一次\n",
    "            label[i] = np.argmax(self.calculate_probabilities(X_test[i]))\n",
    "        \n",
    "        return label\n",
    "    \n",
    "    \n",
    "iris = datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "model = GaussianMixture(n_components=2)\n",
    "model.train(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"IRIS:Number of mislabeled points out of a total %d points : %d, Acc: %f%%\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum(),100*(y_test == y_pred).sum()/X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 利用sk-learn mixture gaussion进行拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4) (75, 4) (75,) (75,)\n",
      "IRIS:Number of mislabeled points out of a total 75 points : 5, Acc: 93.333333%\n"
     ]
    }
   ],
   "source": [
    "class GMM:\n",
    "    def __init__(self):\n",
    "        self.model = {}\n",
    "        self.k = 3\n",
    "        \n",
    "    def EM_GMM(self,X,theta,param,eps=1e-5,max_iter=1000):#eps: 计算精度; max_iter: 最大迭代次数\n",
    "#         print(len(X))#是一个由array构成的list\n",
    "#         print(X)\n",
    "        dpgmm = BayesianGaussianMixture(n_components=param['k']).fit(X)\n",
    "        theta['pi'] = dpgmm.weights_\n",
    "        theta['mu'] = dpgmm.means_\n",
    "        theta['sigma'] = dpgmm.covariances_\n",
    "#         print(theta)\n",
    "        return theta \n",
    "\n",
    "    # 分类别求出数学期望和标准差\n",
    "    def train(self, X, y):\n",
    "        theta = {}; param = {}\n",
    "        param['k'] = self.k; param['N'] = X.shape[0]; param['dim'] = X.shape[1]#这里的N是整个训练样本的样本数，在实际计算时取相同标签的样本数\n",
    "        theta['pi'] = np.ones(param['k'])/param['k']                 # 均匀初始化\n",
    "        theta['mu'] = np.random.random((param['k'],param['dim']))    # 随机初始化\n",
    "        theta['sigma'] = np.array([np.eye(param['dim'])]*param['k']) # 初始化为单位正定矩阵\n",
    "        self.regularization = np.dot(np.eye(param['dim']),0.001)\n",
    "  \n",
    "        labels = list(set(y))#标签的列表\n",
    "        data = {label:[] for label in labels}#{0.0: [], 1.0: []}\n",
    "        for f, label in zip(X, y):\n",
    "            data[label].append(f)#print(data)#形成一个字典，根据标签将训练样本进行分类\n",
    "#         print(data)\n",
    "        \n",
    "        self.model = {label:{} for label in range(len(labels))}\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            dpgmm = BayesianGaussianMixture(n_components=param['k']).fit(data[i])\n",
    "            self.model[i]['pi'] = dpgmm.weights_\n",
    "            self.model[i]['mu'] = dpgmm.means_\n",
    "            self.model[i]['sigma'] = dpgmm.covariances_\n",
    "        \n",
    "#         for label, value in data.items():\n",
    "#             self.model[label] = self.EM_GMM(value,theta,param,eps=1e-5,max_iter=50)\n",
    "#         self.model = {label: self.EM_GMM(value,theta,param,eps=1e-5,max_iter=50) for label, value in data.items()}\n",
    "#         print(self.model)\n",
    "        return self.model\n",
    "\n",
    "    # 计算概率\n",
    "    def calculate_probabilities(self, input_data):\n",
    "        probabilities = {}\n",
    "        dim = np.size(input_data)\n",
    "        \n",
    "        for label, value in self.model.items():#value是一个字典，表示特定标签的模型参数\n",
    "            pp = 0.0\n",
    "            for i in range(self.k):\n",
    "                mu,sigma = value['mu'][i],value['sigma'][i]\n",
    "                pp += multivariate_normal(mu,sigma).pdf(input_data)*value['pi'][i]\n",
    "#                 pp = self.GMM_component(self,input_data,self.model[i]['theta'],self.model[i]['param'],i)\n",
    "#                 probabilities[label] += value['pi'][i]*pp\n",
    "            probabilities[label] = pp\n",
    "            \n",
    "        return probabilities\n",
    "    \n",
    "    # 类别\n",
    "    def predict(self, X_test):\n",
    "        label = list(range(X_test.shape[0]))\n",
    "        for i in range(X_test.shape[0]):#每个样本迭代一次\n",
    "            label[i] = sorted(self.calculate_probabilities(X_test[i,:]).items(), key=lambda x: x[-1])[-1][0]\n",
    "        \n",
    "        return label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合高斯分类鸢尾花数据ISRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRIS:Number of mislabeled points out of a total 75 points : 1, Acc: 98.666667%\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "model = GMM()\n",
    "model.train(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"IRIS:Number of mislabeled points out of a total %d points : %d, Acc: %f%%\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum(),100*(y_test == y_pred).sum()/X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合高斯分类MNIST数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:85: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:85: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPS:Number of mislabeled points out of a total 7000 points : 247, Acc: 96.471429%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA#主成分分析\n",
    "from sklearn.datasets import fetch_mldata\n",
    " \n",
    "mnist = fetch_mldata('MNIST original',data_home=\"E:\\scikit_learn_data\")\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "X_reduced = PCA(n_components=50).fit_transform(X)#特征提取\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_reduced, y, test_size=0.1, random_state=1)#random_state=1使数据集顺序打乱\n",
    "\n",
    "model_d = GMM()\n",
    "model_d.train(X_train_d, y_train_d)\n",
    "y_pred_d = model_d.predict(X_test_d)\n",
    "\n",
    "print(\"USPS:Number of mislabeled points out of a total %d points : %d, Acc: %f%%\"\n",
    "      %(X_test_d.shape[0], (y_test_d != y_pred_d).sum(),100*(y_test_d == y_pred_d).sum()/X_test_d.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合高斯分类CIFAR10数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "C:\\Users\\12569\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPS:Number of mislabeled points out of a total 10000 points : 9035, Acc: 9.650000%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "cifar_batch = {}\n",
    "cifar_batch[0] = unpickle(\"E:\\scikit_learn_data\\cifar-10-batches-py\\data_batch_5\")\n",
    "cifar_batch[1] = unpickle(\"E:\\scikit_learn_data\\cifar-10-batches-py\\data_batch_1\")\n",
    "cifar_batch[2] = unpickle(\"E:\\scikit_learn_data\\cifar-10-batches-py\\data_batch_2\")\n",
    "cifar_batch[3] = unpickle(\"E:\\scikit_learn_data\\cifar-10-batches-py\\data_batch_3\")\n",
    "cifar_batch[4] = unpickle(\"E:\\scikit_learn_data\\cifar-10-batches-py\\data_batch_4\")\n",
    "cifar_test = unpickle(\"E:\\scikit_learn_data\\cifar-10-batches-py\\\\test_batch\")\n",
    "cifar_batch_meta = unpickle(\"E:\\scikit_learn_data\\cifar-10-batches-py\\\\batches.meta\")\n",
    "\n",
    "model_c = GMM()\n",
    "for i in range(4):\n",
    "    X_c,y_c = cifar_batch[i][b'data'],cifar_batch[i][b'labels']\n",
    "    y_c = np.array(y_c)\n",
    "    X_reduced_c = PCA(n_components=50).fit_transform(X_c)#特征提取\n",
    "    model_c.train(X_reduced_c, y_c)\n",
    "\n",
    "X_test,y_test = cifar_batch[4][b'data'],cifar_batch[4][b'labels']\n",
    "y_test_c = np.array(y_test)\n",
    "X_test_c = PCA(n_components=50).fit_transform(X_test)#特征提取\n",
    "y_pred_c = model_d.predict(X_test_c)\n",
    "\n",
    "print(\"USPS:Number of mislabeled points out of a total %d points : %d, Acc: %f%%\"\n",
    "      %(X_test_c.shape[0], (y_test_c != y_pred_c).sum(),100*(y_test_c == y_pred_c).sum()/X_test_c.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
