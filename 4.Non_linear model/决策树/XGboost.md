## XGboost是什么？

转载：[通俗的将Xgboost的原理讲明白](https://www.sohu.com/a/226265476_609569) 

**Xgboost是很多CART回归树集成**

- 概念1：回归树与决策树 事实上，分类与回归是一个型号的东西，只不过分类的结果是离散值，回归是连续的，本质是一样的，都是特征（feature）到结果/标签（label）之间的映射。说说决策树和回归树，在上面决策树的讲解中相信决策树分类已经很好理解了。 回归树是个啥呢？ 直接摘抄人家的一句话，分类树的样本输出（即响应值）是类的形式，如判断蘑菇是有毒还是无毒，周末去看电影还是不去。而回归树的样本输出是数值的形式，比如给某人发放房屋贷款的数额就是具体的数值，可以是0到120万元之间的任意值。 那么，这时候你就没法用上述的信息增益、信息增益率、基尼系数来判定树的节点分裂了，你就会采用新的方式，预测误差，常用的有均方误差、对数误差等。而且节点不再是类别，是数值（预测值），那么怎么确定呢，有的是节点内样本均值，有的是最优化算出来的比如Xgboost。 细节http://blog.csdn.net/app_12062011/article/details/52136117博主讲的不错
- 概念2：boosting集成学习，由多个相关联的决策树联合决策，什么叫相关联，举个例子，有一个样本[数据->标签]是[(2，4，5)-> 4]，第一棵决策树用这个样本训练得预测为3.3，那么第二棵决策树训练时的输入，这个样本就变成了[(2，4，5)-> 0.7]，也就是说，下一棵决策树输入样本会与前面决策树的训练和预测相关。 与之对比的是random foreast（随机森林）算法，各个决策树是独立的、每个决策树在样本堆里随机选一批样本，随机选一批特征进行独立训练，各个决策树之间没有啥毛线关系。 所以首先Xgboost首先是一个boosting的集成学习，这样应该很通俗了
- 这个时候大家就能感觉到一个回归树形成的关键点：（1）分裂点依据什么来划分（如前面说的均方误差最小，loss）；（2）分类后的节点预测值是多少（如前面说，有一种是将叶子节点下各样本实际值得均值作为叶子节点预测误差，或者计算所得）

是时候看看Xgboost了

首先明确下我们的目标，希望建立K个回归树，使得树群的预测值尽量接近真实值（准确率）而且有尽量大的泛化能力（更为本质的东西），从数学角度看这是一个泛函最优化，多目标，看下目标函数：

![img](http://5b0988e595225.cdn.sohucs.com/images/20180324/221dc38e80e34d74b487d7b071907237.jpeg)

直观上看，目标要求预测误差尽量小，叶子节点尽量少，节点数值尽量不极端（这个怎么看，如果某个样本label数值为4，那么第一个回归树预测3，第二个预测为1；另外一组回归树，一个预测2，一个预测2，那么倾向后一种，为什么呢？前一种情况，第一棵树学的太多，太接近4，也就意味着有较大的过拟合的风险）

ok，听起来很美好，可是怎么实现呢，上面这个目标函数跟实际的参数怎么联系起来，记得我们说过，回归树的参数:（1）选取哪个feature分裂节点呢；（2）节点的预测值（总不能靠取平均值这么粗暴不讲道理的方式吧，好歹高级一点）。上述形而上的公式并没有“直接”解决这两个，那么是如何间接解决的呢？

先说答案：贪心策略+最优化（二次最优化，恩你没看错）

通俗解释贪心策略：就是决策时刻按照当前目标最优化决定，说白了就是眼前利益最大化决定，“目光短浅”策略，他的优缺点细节大家自己去了解，经典背包问题等等。

这里是怎么用贪心策略的呢，刚开始你有一群样本，放在第一个节点，这时候T = 1 ，w多少呢，不知道，是求出来的，这时候所有样本的预测值都是w（这个地方自己好好理解，决策树的节点表示类别，回归树的节点表示预测值）,带入样本的label数值，此时loss function变为

![img](http://5b0988e595225.cdn.sohucs.com/images/20180324/77f4d47431c94c53b7bc59da46ebf7ec.png)

暂停下，这里你发现了没，二次函数最优化！

要是损失函数不是二次函数咋办，哦，泰勒展开式会否？，不是二次的想办法近似为二次。

接着来，接下来要选个feature分裂成两个节点，变成一棵弱小的树苗，那么需要：（1）确定分裂用的feature，how？最简单的是粗暴的枚举，选择loss function效果最好的那个（关于粗暴枚举，Xgboost的改良并行方式咱们后面看）；（2）如何确立节点的ww 以及最小的loss function，大声告诉我怎么做？对，二次函数的求最值（细节的会注意到，计算二次最值是不是有固定套路，导数=0的点，ok）

那么节奏是，选择一个feature分裂，计算loss function最小值，然后再选一个feature分裂，又得到一个loss function最小值…你枚举完，找一个效果最好的，把树给分裂，就得到了小树苗。

在分裂的时候，你可以注意到，每次节点分裂，loss function被影响的只有这个节点的样本，因而每次分裂，计算分裂的增益（loss function的降低量）只需要关注打算分裂的那个节点的样本。原论文这里会推导出一个优雅的公式，我不想敲latex公式了，

![img](http://5b0988e595225.cdn.sohucs.com/images/20180324/ad9680526adb460291c5acb3b5496008.jpeg)

想研究公式的去这里吧

http://matafight.github.io/2017/03/14/XGBoost-%E7%AE%80%E4%BB%8B/

接下来，继续分裂，按照上述的方式，形成一棵树，再形成一棵树，每次在上一次的预测基础上取最优进一步分裂/建树，是不是贪心策略？！

凡是这种循环迭代的方式必定有停止条件，什么时候停止呢：

（1）当引入的分裂带来的增益小于一个阀值的时候，我们可以剪掉这个分裂，所以并不是每一次分裂loss function整体都会增加的，有点预剪枝的意思（其实我这里有点疑问的，一般后剪枝效果比预剪枝要好点吧，只不过复杂麻烦些，这里大神请指教，为啥这里使用的是预剪枝的思想，当然Xgboost支持后剪枝），阈值参数为γγ 正则项里叶子节点数T的系数（大神请确认下）；

（2）当树达到最大深度时则停止建立决策树，设置一个超参数max_depth，这个好理解吧，树太深很容易出现的情况学习局部样本，过拟合；

（3）当样本权重和小于设定阈值时则停止建树，这个解释一下，涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的 min_child_leaf 参数类似，但不完全一样，大意就是一个叶子节点样本太少了，也终止同样是过拟合；

**看下Xgboost的一些重点**

- w是最优化求出来的，不是啥平均值或规则指定的，这个算是一个思路上的新颖吧；
- 正则化防止过拟合的技术，上述看到了，直接loss function里面就有；
- 支持自定义loss function，哈哈，不用我多说，只要能泰勒展开（能求一阶导和二阶导）就行，你开心就好；
- 支持并行化，这个地方有必要说明下，因为这是xgboost的闪光点，直接的效果是训练速度快，boosting技术中下一棵树依赖上述树的训练和预测，所以树与树之间应该是只能串行！那么大家想想，哪里可以并行？！ 没错，在选择最佳分裂点，进行枚举的时候并行！（据说恰好这个也是树形成最耗时的阶段） Attention：同层级节点可并行。具体的对于某个节点，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。—– 较少的离散值作为分割点倒是很简单，比如“是否是单身”来分裂节点计算增益是很easy，但是“月收入”这种feature，取值很多，从5k~50k都有，总不可能每个分割点都来试一下计算分裂增益吧？（比如月收入feature有1000个取值，难道你把这1000个用作分割候选？缺点1：计算量，缺点2：出现叶子节点样本过少，过拟合）我们常用的习惯就是划分区间，那么问题来了，这个区间分割点如何确定（难道平均分割），作者是这么做的： 方法名字：Weighted Quantile Sketch 大家还记得每个样本在节点（将要分裂的节点）处的loss function一阶导数 gi g i 和二阶导数 hi h i ，衡量预测值变化带来的loss function变化，举例来说，将样本“月收入”进行升序排列，5k、5.2k、5.3k、…、52k，分割线为“收入1”、“收入2”、…、“收入j”，满足(每个间隔的样本的 hi h i 之和/总样本的 hi h i 之和）为某个百分比 ϵ ϵ （我这个是近似的说法），那么可以一共分成大约 1 / ϵ 个分裂点。